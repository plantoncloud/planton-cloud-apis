// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: cloud/planton/apis/v1/code2cloud/deploy/kafka/rpc/model.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

///kafka-cluster
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaCluster {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  ///resource api-version
  public var apiVersion: String = String()

  ///resource kind
  public var kind: String = String()

  ///resource metadata
  public var metadata: Cloud_Planton_Apis_V1_Commons_Resource_Metadata {
    get {return _metadata ?? Cloud_Planton_Apis_V1_Commons_Resource_Metadata()}
    set {_metadata = newValue}
  }
  /// Returns true if `metadata` has been explicitly set.
  public var hasMetadata: Bool {return self._metadata != nil}
  /// Clears the value of `metadata`. Subsequent reads from it will return its default value.
  public mutating func clearMetadata() {self._metadata = nil}

  ///spec
  public var spec: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpec {
    get {return _spec ?? Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpec()}
    set {_spec = newValue}
  }
  /// Returns true if `spec` has been explicitly set.
  public var hasSpec: Bool {return self._spec != nil}
  /// Clears the value of `spec`. Subsequent reads from it will return its default value.
  public mutating func clearSpec() {self._spec = nil}

  ///status
  public var status: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatus {
    get {return _status ?? Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatus()}
    set {_status = newValue}
  }
  /// Returns true if `status` has been explicitly set.
  public var hasStatus: Bool {return self._status != nil}
  /// Clears the value of `status`. Subsequent reads from it will return its default value.
  public mutating func clearStatus() {self._status = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _metadata: Cloud_Planton_Apis_V1_Commons_Resource_Metadata? = nil
  fileprivate var _spec: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpec? = nil
  fileprivate var _status: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatus? = nil
}

///kafka-cluster spec
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpec {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  ///resource parent
  public var owner: Cloud_Planton_Apis_V1_Commons_Resource_Owner_EnvironmentResourceOwner {
    get {return _owner ?? Cloud_Planton_Apis_V1_Commons_Resource_Owner_EnvironmentResourceOwner()}
    set {_owner = newValue}
  }
  /// Returns true if `owner` has been explicitly set.
  public var hasOwner: Bool {return self._owner != nil}
  /// Clears the value of `owner`. Subsequent reads from it will return its default value.
  public mutating func clearOwner() {self._owner = nil}

  ///list of kafka topics.
  public var kafkaTopics: [Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic] = []

  ///kubernetes spec
  public var kubernetes: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpec {
    get {return _kubernetes ?? Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpec()}
    set {_kubernetes = newValue}
  }
  /// Returns true if `kubernetes` has been explicitly set.
  public var hasKubernetes: Bool {return self._kubernetes != nil}
  /// Clears the value of `kubernetes`. Subsequent reads from it will return its default value.
  public mutating func clearKubernetes() {self._kubernetes = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _owner: Cloud_Planton_Apis_V1_Commons_Resource_Owner_EnvironmentResourceOwner? = nil
  fileprivate var _kubernetes: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpec? = nil
}

///kafka-cluster status
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatus {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// resource lifecycle
  public var lifecycle: Cloud_Planton_Apis_V1_Commons_Resource_RunnableResourceLifecycle {
    get {return _storage._lifecycle ?? Cloud_Planton_Apis_V1_Commons_Resource_RunnableResourceLifecycle()}
    set {_uniqueStorage()._lifecycle = newValue}
  }
  /// Returns true if `lifecycle` has been explicitly set.
  public var hasLifecycle: Bool {return _storage._lifecycle != nil}
  /// Clears the value of `lifecycle`. Subsequent reads from it will return its default value.
  public mutating func clearLifecycle() {_uniqueStorage()._lifecycle = nil}

  /// system audit info
  public var sysAudit: Cloud_Planton_Apis_V1_Commons_Audit_SysAudit {
    get {return _storage._sysAudit ?? Cloud_Planton_Apis_V1_Commons_Audit_SysAudit()}
    set {_uniqueStorage()._sysAudit = newValue}
  }
  /// Returns true if `sysAudit` has been explicitly set.
  public var hasSysAudit: Bool {return _storage._sysAudit != nil}
  /// Clears the value of `sysAudit`. Subsequent reads from it will return its default value.
  public mutating func clearSysAudit() {_uniqueStorage()._sysAudit = nil}

  /// id of the stack-job
  public var stackJobID: String {
    get {return _storage._stackJobID}
    set {_uniqueStorage()._stackJobID = newValue}
  }

  ///kafka-cluster kubernetes status
  public var kubernetes: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatusKubernetesStatus {
    get {return _storage._kubernetes ?? Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatusKubernetesStatus()}
    set {_uniqueStorage()._kubernetes = newValue}
  }
  /// Returns true if `kubernetes` has been explicitly set.
  public var hasKubernetes: Bool {return _storage._kubernetes != nil}
  /// Clears the value of `kubernetes`. Subsequent reads from it will return its default value.
  public mutating func clearKubernetes() {_uniqueStorage()._kubernetes = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

///kafka-cluster kubernetes spec
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpec {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  ///kafka-broker container spec
  public var kafkaBrokerContainer: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecKafkaBrokerContainerSpec {
    get {return _storage._kafkaBrokerContainer ?? Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecKafkaBrokerContainerSpec()}
    set {_uniqueStorage()._kafkaBrokerContainer = newValue}
  }
  /// Returns true if `kafkaBrokerContainer` has been explicitly set.
  public var hasKafkaBrokerContainer: Bool {return _storage._kafkaBrokerContainer != nil}
  /// Clears the value of `kafkaBrokerContainer`. Subsequent reads from it will return its default value.
  public mutating func clearKafkaBrokerContainer() {_uniqueStorage()._kafkaBrokerContainer = nil}

  ///zookeeper container spec
  public var zookeeperContainer: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecZookeeperContainerSpec {
    get {return _storage._zookeeperContainer ?? Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecZookeeperContainerSpec()}
    set {_uniqueStorage()._zookeeperContainer = newValue}
  }
  /// Returns true if `zookeeperContainer` has been explicitly set.
  public var hasZookeeperContainer: Bool {return _storage._zookeeperContainer != nil}
  /// Clears the value of `zookeeperContainer`. Subsequent reads from it will return its default value.
  public mutating func clearZookeeperContainer() {_uniqueStorage()._zookeeperContainer = nil}

  ///schema-registry container spec
  public var schemaRegistryContainer: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecSchemaRegistryContainerSpec {
    get {return _storage._schemaRegistryContainer ?? Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecSchemaRegistryContainerSpec()}
    set {_uniqueStorage()._schemaRegistryContainer = newValue}
  }
  /// Returns true if `schemaRegistryContainer` has been explicitly set.
  public var hasSchemaRegistryContainer: Bool {return _storage._schemaRegistryContainer != nil}
  /// Clears the value of `schemaRegistryContainer`. Subsequent reads from it will return its default value.
  public mutating func clearSchemaRegistryContainer() {_uniqueStorage()._schemaRegistryContainer = nil}

  ///kafka-cluster ingress spec
  public var ingress: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecIngressSpec {
    get {return _storage._ingress ?? Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecIngressSpec()}
    set {_uniqueStorage()._ingress = newValue}
  }
  /// Returns true if `ingress` has been explicitly set.
  public var hasIngress: Bool {return _storage._ingress != nil}
  /// Clears the value of `ingress`. Subsequent reads from it will return its default value.
  public mutating func clearIngress() {_uniqueStorage()._ingress = nil}

  ///flag to control if kowl dashboard is deployed for the kafka-cluster.
  ///defaults to "false".
  public var isKowlDashboardEnabled: Bool {
    get {return _storage._isKowlDashboardEnabled}
    set {_uniqueStorage()._isKowlDashboardEnabled = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

///kafka-cluster kubernetes kafka-broker spec
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecKafkaBrokerContainerSpec {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  ///(optional for create) number of brokers required to setup kafka-cluster.
  ///defaults value "1" is set if client sets the value to 0.
  ///recommended default value is "1".
  public var replicas: Int32 = 0

  ///kafka broker container cpu and memory resources.
  ///recommended default "cpu-requests: 50m, memory-requests: 256Mi, cpu-limits: 1, memory-limits: 1Gi"
  public var resources: Cloud_Planton_Apis_V1_Commons_Kubernetes_ContainerResources {
    get {return _resources ?? Cloud_Planton_Apis_V1_Commons_Kubernetes_ContainerResources()}
    set {_resources = newValue}
  }
  /// Returns true if `resources` has been explicitly set.
  public var hasResources: Bool {return self._resources != nil}
  /// Clears the value of `resources`. Subsequent reads from it will return its default value.
  public mutating func clearResources() {self._resources = nil}

  ///size of the disk to be attached to each broker instance. ex: 30Gi
  ///defaults value is set if not provided by the client.
  public var diskSize: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _resources: Cloud_Planton_Apis_V1_Commons_Kubernetes_ContainerResources? = nil
}

///kafka-cluster kubernetes zookeeper spec
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecZookeeperContainerSpec {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  ///number or zookeeper container replicas
  ///zookeeper requires latest 3 replicas for high availability(ha) mode.
  ///zookeeper is built using raft consensus algorithm.
  ///refer to https://raft.github.io/ to learn more on how replica count affect availability.
  public var replicas: Int32 = 0

  ///zookeeper container cpu and memory resources.
  ///recommended default "cpu-requests: 50m, memory-requests: 256Mi, cpu-limits: 1, memory-limits: 1Gi"
  public var resources: Cloud_Planton_Apis_V1_Commons_Kubernetes_ContainerResources {
    get {return _resources ?? Cloud_Planton_Apis_V1_Commons_Kubernetes_ContainerResources()}
    set {_resources = newValue}
  }
  /// Returns true if `resources` has been explicitly set.
  public var hasResources: Bool {return self._resources != nil}
  /// Clears the value of `resources`. Subsequent reads from it will return its default value.
  public mutating func clearResources() {self._resources = nil}

  ///size of the disk to be attached to each zookeeper instance. ex: 30Gi
  ///defaults value is set if not provided by the client.
  public var diskSize: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _resources: Cloud_Planton_Apis_V1_Commons_Kubernetes_ContainerResources? = nil
}

///kafka-cluster kubernetes schema-registry spec
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecSchemaRegistryContainerSpec {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  ///flag to control if schema registry is created for the kafka-cluster.
  ///defaults to "false".
  public var isEnabled: Bool = false

  ///number of schema registry replicas.
  ///recommended default value is "1".
  ///this value has no effect if the is_schema_registry_enabled is set to false.
  public var replicas: Int32 = 0

  ///schema-registry container cpu and memory resources.
  ///recommended default "cpu-requests: 50m, memory-requests: 256Mi, cpu-limits: 1, memory-limits: 1Gi"
  public var resources: Cloud_Planton_Apis_V1_Commons_Kubernetes_ContainerResources {
    get {return _resources ?? Cloud_Planton_Apis_V1_Commons_Kubernetes_ContainerResources()}
    set {_resources = newValue}
  }
  /// Returns true if `resources` has been explicitly set.
  public var hasResources: Bool {return self._resources != nil}
  /// Clears the value of `resources`. Subsequent reads from it will return its default value.
  public mutating func clearResources() {self._resources = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _resources: Cloud_Planton_Apis_V1_Commons_Kubernetes_ContainerResources? = nil
}

///kafka-cluster kubernetes ingress spec
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecIngressSpec {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  ///standard-endpoint domain to be used for creating internal and external endpoints for kafka-cluster.
  ///only tls enabled standard-endpoints are eligible for creating kafka endpoints.
  public var standardEndpointID: String = String()

  ///endpoint-domain-name used for creating kafka-cluster endpoints.
  ///value is computed from the configured standard-endpoint.
  public var endpointDomainName: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

///kafka-cluster kubernetes status
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatusKubernetesStatus {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  ///name of the kubernetes namespace in which the kafka-cluster is created.
  public var namespace: String = String()

  ///sasl user name of kafka-cluster.
  ///username will be automatically set as 'admin' while creating the kafka-cluster.
  public var kafkaSaslUsername: String = String()

  ///external hostname of kafka bootstrap server.
  public var externalBootstrapServerHostname: String = String()

  ///internal hostname of kafka bootstrap server.
  public var internalBootstrapServerHostname: String = String()

  ///external url of schema registry.
  ///this is set to empty when schema registry is not enabled.
  public var externalSchemaRegistryURL: String = String()

  ///internal url of schema registry.
  ///this is set to empty when schema registry is not enabled.
  public var internalSchemaRegistryURL: String = String()

  ///external url to access kowl dashboard.
  ///this is set to empty when kowl dashboard is not enabled.
  public var externalKowlDashboardURL: String = String()

  ///internal url to access kowl dashboard.
  ///this is set to empty when kowl dashboard is not enabled.
  public var internalKowlDashboardURL: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

///wrapper for id field of kafka-cluster
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterId {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var value: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

///list of kafka-clusters
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusters {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var entries: [Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaCluster] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

///kafka-topic
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var sysAudit: Cloud_Planton_Apis_V1_Commons_Audit_SysAudit {
    get {return _sysAudit ?? Cloud_Planton_Apis_V1_Commons_Audit_SysAudit()}
    set {_sysAudit = newValue}
  }
  /// Returns true if `sysAudit` has been explicitly set.
  public var hasSysAudit: Bool {return self._sysAudit != nil}
  /// Clears the value of `sysAudit`. Subsequent reads from it will return its default value.
  public mutating func clearSysAudit() {self._sysAudit = nil}

  ///topic name
  public var name: String = String()

  ///topic id
  public var id: String = String()

  ///topic partitions.
  ///recommended default is 1.
  public var partitions: Int32 = 0

  ///topic replicas.
  ///recommended default is 1.
  public var replicas: Int32 = 0

  ///additional configuration of kafka topic
  ///if not provided then default values will be set
  ///for example default delete.policy is `delete` and can be set up as `compact`
  public var config: Dictionary<String,String> = [:]

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _sysAudit: Cloud_Planton_Apis_V1_Commons_Audit_SysAudit? = nil
}

///wrapper for kafka topic config
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var value: Dictionary<String,String> = [:]

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

///list of kafka topics
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopics {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var entries: [Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

///wrapper for kafka topic id
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicId {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var value: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

///wrapper for kafka-cluster password
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterPassword {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var value: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

///input for command to add multiple kafka topics to a kafka-cluster
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_AddKafkaTopicsCommandInput {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  ///id of the kafka-cluster to which the kafka topics are added
  public var kafkaClusterID: String = String()

  ///list of kafka topics to be added to existing list of kafka topics
  public var kafkaTopics: [Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

///response for paginated query to list kafka-clusters
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterList {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var totalPages: Int32 = 0

  public var entries: [Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaCluster] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

///AddOrUpdateKafkaTopicCommandInput is used to encapsulate the details required
///for adding a new kafka-topic to a specific kafka-cluster, or updating
///an existing one. This message is typically used to transmit data between the client and
///server during an add or update operation concerning a specific kafka-topic
///associated with a particular kafka-cluster.
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_AddOrUpdateKafkaTopicCommandInput {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  ///The unique identifier for the kafka-cluster to which the kafka-topic
  /// needs to be added or updated. This field must be populated with a valid
  ///kafka-cluster ID, which can be obtained from the kafka-cluster entity itself.
  ///The server uses this ID to identify the correct kafka-cluster where the
  ///kafka-topic needs to be added or updated.
  public var kafkaClusterID: String = String()

  ///The kafka-topic that needs to be added or updated within the product
  ///environment. This field should be populated with a valid KafkaTopic object,
  ///which encapsulates the details of the kafka-topic. If an kafka-topic
  ///with the kafka-topic-id already exists in the kafka-cluster, the value will be updated.
  ///Otherwise, a new kafka-topic will be created with the provided details.
  public var kafkaTopic: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic {
    get {return _kafkaTopic ?? Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic()}
    set {_kafkaTopic = newValue}
  }
  /// Returns true if `kafkaTopic` has been explicitly set.
  public var hasKafkaTopic: Bool {return self._kafkaTopic != nil}
  /// Clears the value of `kafkaTopic`. Subsequent reads from it will return its default value.
  public mutating func clearKafkaTopic() {self._kafkaTopic = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _kafkaTopic: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic? = nil
}

///DeleteOrRestoreKafkaTopicCommandInput is used to encapsulate the details required for
///deleting or restoring a kafka-topic of a specific kafka-cluster.
///This message is typically used to transmit data between the client and the server
///during a delete or restore operation concerning a specific kafka-topic associated
///with a particular kafka-cluster.
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_DeleteOrRestoreKafkaTopicCommandInput {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  ///The unique identifier for the kafka-cluster from which the kafka-topic
  /// needs to be deleted or restored. This field must be populated with a valid
  ///kafka-cluster ID, which can be obtained from the kafka-cluster entity itself.
  ///The server uses this ID to identify the correct kafka-cluster from which
  ///the kafka-topic needs to be deleted or restored.
  public var kafkaClusterID: String = String()

  ///The kafka-topic-id of the kafka-topic that needs to be deleted or restored.
  ///This field should be populated with a valid
  ///kafka-topic-id, which can be obtained from the kafka-topic entity itself.
  ///The server uses this kafka-topic-id to identify the correct kafka-topic that
  ///needs to be deleted or restored.
  public var kafkaTopicID: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

///KafkaTopicQueryInput is a message type that serves as input for queries
///related to Kafka topics within a specific Kafka cluster.
///It contains information about the specific Kafka cluster and the Kafka
///topic to be queried.
public struct Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicQueryInput {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  ///Unique identifier of the Kafka cluster from which the Kafka topic
  ///information is to be retrieved.
  ///This field is required, as specified by the is_required field option.
  public var kafkaClusterID: String = String()

  ///KafkaTopic object that represents the Kafka topic to be queried
  ///in the Kafka cluster.
  ///This encapsulates all the necessary information about the Kafka topic.
  public var kafkaTopic: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic {
    get {return _kafkaTopic ?? Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic()}
    set {_kafkaTopic = newValue}
  }
  /// Returns true if `kafkaTopic` has been explicitly set.
  public var hasKafkaTopic: Bool {return self._kafkaTopic != nil}
  /// Clears the value of `kafkaTopic`. Subsequent reads from it will return its default value.
  public mutating func clearKafkaTopic() {self._kafkaTopic = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _kafkaTopic: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic? = nil
}

#if swift(>=5.5) && canImport(_Concurrency)
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaCluster: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpec: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatus: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpec: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecKafkaBrokerContainerSpec: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecZookeeperContainerSpec: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecSchemaRegistryContainerSpec: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecIngressSpec: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatusKubernetesStatus: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterId: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusters: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicConfig: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopics: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicId: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterPassword: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_AddKafkaTopicsCommandInput: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterList: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_AddOrUpdateKafkaTopicCommandInput: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_DeleteOrRestoreKafkaTopicCommandInput: @unchecked Sendable {}
extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicQueryInput: @unchecked Sendable {}
#endif  // swift(>=5.5) && canImport(_Concurrency)

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "cloud.planton.apis.v1.code2cloud.deploy.kafka.rpc"

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaCluster: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaCluster"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "api_version"),
    2: .same(proto: "kind"),
    3: .same(proto: "metadata"),
    4: .same(proto: "spec"),
    5: .same(proto: "status"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.apiVersion) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.kind) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._metadata) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._spec) }()
      case 5: try { try decoder.decodeSingularMessageField(value: &self._status) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.apiVersion.isEmpty {
      try visitor.visitSingularStringField(value: self.apiVersion, fieldNumber: 1)
    }
    if !self.kind.isEmpty {
      try visitor.visitSingularStringField(value: self.kind, fieldNumber: 2)
    }
    try { if let v = self._metadata {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    try { if let v = self._spec {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    } }()
    try { if let v = self._status {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaCluster, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaCluster) -> Bool {
    if lhs.apiVersion != rhs.apiVersion {return false}
    if lhs.kind != rhs.kind {return false}
    if lhs._metadata != rhs._metadata {return false}
    if lhs._spec != rhs._spec {return false}
    if lhs._status != rhs._status {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpec: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaClusterSpec"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "owner"),
    2: .standard(proto: "kafka_topics"),
    3: .same(proto: "kubernetes"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._owner) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.kafkaTopics) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._kubernetes) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._owner {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    if !self.kafkaTopics.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.kafkaTopics, fieldNumber: 2)
    }
    try { if let v = self._kubernetes {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpec, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpec) -> Bool {
    if lhs._owner != rhs._owner {return false}
    if lhs.kafkaTopics != rhs.kafkaTopics {return false}
    if lhs._kubernetes != rhs._kubernetes {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatus: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaClusterStatus"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    99: .same(proto: "lifecycle"),
    98: .standard(proto: "sys_audit"),
    97: .standard(proto: "stack_job_id"),
    1: .same(proto: "kubernetes"),
  ]

  fileprivate class _StorageClass {
    var _lifecycle: Cloud_Planton_Apis_V1_Commons_Resource_RunnableResourceLifecycle? = nil
    var _sysAudit: Cloud_Planton_Apis_V1_Commons_Audit_SysAudit? = nil
    var _stackJobID: String = String()
    var _kubernetes: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatusKubernetesStatus? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _lifecycle = source._lifecycle
      _sysAudit = source._sysAudit
      _stackJobID = source._stackJobID
      _kubernetes = source._kubernetes
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularMessageField(value: &_storage._kubernetes) }()
        case 97: try { try decoder.decodeSingularStringField(value: &_storage._stackJobID) }()
        case 98: try { try decoder.decodeSingularMessageField(value: &_storage._sysAudit) }()
        case 99: try { try decoder.decodeSingularMessageField(value: &_storage._lifecycle) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      try { if let v = _storage._kubernetes {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      } }()
      if !_storage._stackJobID.isEmpty {
        try visitor.visitSingularStringField(value: _storage._stackJobID, fieldNumber: 97)
      }
      try { if let v = _storage._sysAudit {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 98)
      } }()
      try { if let v = _storage._lifecycle {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 99)
      } }()
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatus, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatus) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._lifecycle != rhs_storage._lifecycle {return false}
        if _storage._sysAudit != rhs_storage._sysAudit {return false}
        if _storage._stackJobID != rhs_storage._stackJobID {return false}
        if _storage._kubernetes != rhs_storage._kubernetes {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpec: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaClusterSpecKubernetesSpec"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "kafka_broker_container"),
    2: .standard(proto: "zookeeper_container"),
    3: .standard(proto: "schema_registry_container"),
    4: .same(proto: "ingress"),
    5: .standard(proto: "is_kowl_dashboard_enabled"),
  ]

  fileprivate class _StorageClass {
    var _kafkaBrokerContainer: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecKafkaBrokerContainerSpec? = nil
    var _zookeeperContainer: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecZookeeperContainerSpec? = nil
    var _schemaRegistryContainer: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecSchemaRegistryContainerSpec? = nil
    var _ingress: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecIngressSpec? = nil
    var _isKowlDashboardEnabled: Bool = false

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _kafkaBrokerContainer = source._kafkaBrokerContainer
      _zookeeperContainer = source._zookeeperContainer
      _schemaRegistryContainer = source._schemaRegistryContainer
      _ingress = source._ingress
      _isKowlDashboardEnabled = source._isKowlDashboardEnabled
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularMessageField(value: &_storage._kafkaBrokerContainer) }()
        case 2: try { try decoder.decodeSingularMessageField(value: &_storage._zookeeperContainer) }()
        case 3: try { try decoder.decodeSingularMessageField(value: &_storage._schemaRegistryContainer) }()
        case 4: try { try decoder.decodeSingularMessageField(value: &_storage._ingress) }()
        case 5: try { try decoder.decodeSingularBoolField(value: &_storage._isKowlDashboardEnabled) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      try { if let v = _storage._kafkaBrokerContainer {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      } }()
      try { if let v = _storage._zookeeperContainer {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      } }()
      try { if let v = _storage._schemaRegistryContainer {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      } }()
      try { if let v = _storage._ingress {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      } }()
      if _storage._isKowlDashboardEnabled != false {
        try visitor.visitSingularBoolField(value: _storage._isKowlDashboardEnabled, fieldNumber: 5)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpec, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpec) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._kafkaBrokerContainer != rhs_storage._kafkaBrokerContainer {return false}
        if _storage._zookeeperContainer != rhs_storage._zookeeperContainer {return false}
        if _storage._schemaRegistryContainer != rhs_storage._schemaRegistryContainer {return false}
        if _storage._ingress != rhs_storage._ingress {return false}
        if _storage._isKowlDashboardEnabled != rhs_storage._isKowlDashboardEnabled {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecKafkaBrokerContainerSpec: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaClusterSpecKubernetesSpecKafkaBrokerContainerSpec"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "replicas"),
    2: .same(proto: "resources"),
    3: .standard(proto: "disk_size"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt32Field(value: &self.replicas) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._resources) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.diskSize) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if self.replicas != 0 {
      try visitor.visitSingularInt32Field(value: self.replicas, fieldNumber: 1)
    }
    try { if let v = self._resources {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    if !self.diskSize.isEmpty {
      try visitor.visitSingularStringField(value: self.diskSize, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecKafkaBrokerContainerSpec, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecKafkaBrokerContainerSpec) -> Bool {
    if lhs.replicas != rhs.replicas {return false}
    if lhs._resources != rhs._resources {return false}
    if lhs.diskSize != rhs.diskSize {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecZookeeperContainerSpec: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaClusterSpecKubernetesSpecZookeeperContainerSpec"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "replicas"),
    2: .same(proto: "resources"),
    3: .standard(proto: "disk_size"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt32Field(value: &self.replicas) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._resources) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.diskSize) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if self.replicas != 0 {
      try visitor.visitSingularInt32Field(value: self.replicas, fieldNumber: 1)
    }
    try { if let v = self._resources {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    if !self.diskSize.isEmpty {
      try visitor.visitSingularStringField(value: self.diskSize, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecZookeeperContainerSpec, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecZookeeperContainerSpec) -> Bool {
    if lhs.replicas != rhs.replicas {return false}
    if lhs._resources != rhs._resources {return false}
    if lhs.diskSize != rhs.diskSize {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecSchemaRegistryContainerSpec: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaClusterSpecKubernetesSpecSchemaRegistryContainerSpec"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "is_enabled"),
    2: .same(proto: "replicas"),
    3: .same(proto: "resources"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularBoolField(value: &self.isEnabled) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.replicas) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._resources) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if self.isEnabled != false {
      try visitor.visitSingularBoolField(value: self.isEnabled, fieldNumber: 1)
    }
    if self.replicas != 0 {
      try visitor.visitSingularInt32Field(value: self.replicas, fieldNumber: 2)
    }
    try { if let v = self._resources {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecSchemaRegistryContainerSpec, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecSchemaRegistryContainerSpec) -> Bool {
    if lhs.isEnabled != rhs.isEnabled {return false}
    if lhs.replicas != rhs.replicas {return false}
    if lhs._resources != rhs._resources {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecIngressSpec: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaClusterSpecKubernetesSpecIngressSpec"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "standard_endpoint_id"),
    2: .standard(proto: "endpoint_domain_name"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.standardEndpointID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.endpointDomainName) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.standardEndpointID.isEmpty {
      try visitor.visitSingularStringField(value: self.standardEndpointID, fieldNumber: 1)
    }
    if !self.endpointDomainName.isEmpty {
      try visitor.visitSingularStringField(value: self.endpointDomainName, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecIngressSpec, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterSpecKubernetesSpecIngressSpec) -> Bool {
    if lhs.standardEndpointID != rhs.standardEndpointID {return false}
    if lhs.endpointDomainName != rhs.endpointDomainName {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatusKubernetesStatus: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaClusterStatusKubernetesStatus"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "namespace"),
    2: .standard(proto: "kafka_sasl_username"),
    3: .standard(proto: "external_bootstrap_server_hostname"),
    4: .standard(proto: "internal_bootstrap_server_hostname"),
    5: .standard(proto: "external_schema_registry_url"),
    6: .standard(proto: "internal_schema_registry_url"),
    7: .standard(proto: "external_kowl_dashboard_url"),
    8: .standard(proto: "internal_kowl_dashboard_url"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.namespace) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.kafkaSaslUsername) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.externalBootstrapServerHostname) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.internalBootstrapServerHostname) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.externalSchemaRegistryURL) }()
      case 6: try { try decoder.decodeSingularStringField(value: &self.internalSchemaRegistryURL) }()
      case 7: try { try decoder.decodeSingularStringField(value: &self.externalKowlDashboardURL) }()
      case 8: try { try decoder.decodeSingularStringField(value: &self.internalKowlDashboardURL) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.namespace.isEmpty {
      try visitor.visitSingularStringField(value: self.namespace, fieldNumber: 1)
    }
    if !self.kafkaSaslUsername.isEmpty {
      try visitor.visitSingularStringField(value: self.kafkaSaslUsername, fieldNumber: 2)
    }
    if !self.externalBootstrapServerHostname.isEmpty {
      try visitor.visitSingularStringField(value: self.externalBootstrapServerHostname, fieldNumber: 3)
    }
    if !self.internalBootstrapServerHostname.isEmpty {
      try visitor.visitSingularStringField(value: self.internalBootstrapServerHostname, fieldNumber: 4)
    }
    if !self.externalSchemaRegistryURL.isEmpty {
      try visitor.visitSingularStringField(value: self.externalSchemaRegistryURL, fieldNumber: 5)
    }
    if !self.internalSchemaRegistryURL.isEmpty {
      try visitor.visitSingularStringField(value: self.internalSchemaRegistryURL, fieldNumber: 6)
    }
    if !self.externalKowlDashboardURL.isEmpty {
      try visitor.visitSingularStringField(value: self.externalKowlDashboardURL, fieldNumber: 7)
    }
    if !self.internalKowlDashboardURL.isEmpty {
      try visitor.visitSingularStringField(value: self.internalKowlDashboardURL, fieldNumber: 8)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatusKubernetesStatus, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterStatusKubernetesStatus) -> Bool {
    if lhs.namespace != rhs.namespace {return false}
    if lhs.kafkaSaslUsername != rhs.kafkaSaslUsername {return false}
    if lhs.externalBootstrapServerHostname != rhs.externalBootstrapServerHostname {return false}
    if lhs.internalBootstrapServerHostname != rhs.internalBootstrapServerHostname {return false}
    if lhs.externalSchemaRegistryURL != rhs.externalSchemaRegistryURL {return false}
    if lhs.internalSchemaRegistryURL != rhs.internalSchemaRegistryURL {return false}
    if lhs.externalKowlDashboardURL != rhs.externalKowlDashboardURL {return false}
    if lhs.internalKowlDashboardURL != rhs.internalKowlDashboardURL {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterId: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaClusterId"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "value"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.value) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.value.isEmpty {
      try visitor.visitSingularStringField(value: self.value, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterId, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterId) -> Bool {
    if lhs.value != rhs.value {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusters: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaClusters"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "entries"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.entries) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.entries.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.entries, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusters, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusters) -> Bool {
    if lhs.entries != rhs.entries {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaTopic"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    99: .standard(proto: "sys_audit"),
    1: .same(proto: "name"),
    2: .same(proto: "id"),
    4: .same(proto: "partitions"),
    5: .same(proto: "replicas"),
    6: .same(proto: "config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.id) }()
      case 4: try { try decoder.decodeSingularInt32Field(value: &self.partitions) }()
      case 5: try { try decoder.decodeSingularInt32Field(value: &self.replicas) }()
      case 6: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.config) }()
      case 99: try { try decoder.decodeSingularMessageField(value: &self._sysAudit) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if !self.id.isEmpty {
      try visitor.visitSingularStringField(value: self.id, fieldNumber: 2)
    }
    if self.partitions != 0 {
      try visitor.visitSingularInt32Field(value: self.partitions, fieldNumber: 4)
    }
    if self.replicas != 0 {
      try visitor.visitSingularInt32Field(value: self.replicas, fieldNumber: 5)
    }
    if !self.config.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.config, fieldNumber: 6)
    }
    try { if let v = self._sysAudit {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 99)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopic) -> Bool {
    if lhs._sysAudit != rhs._sysAudit {return false}
    if lhs.name != rhs.name {return false}
    if lhs.id != rhs.id {return false}
    if lhs.partitions != rhs.partitions {return false}
    if lhs.replicas != rhs.replicas {return false}
    if lhs.config != rhs.config {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaTopicConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "value"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.value) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.value.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.value, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicConfig, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicConfig) -> Bool {
    if lhs.value != rhs.value {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopics: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaTopics"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "entries"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.entries) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.entries.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.entries, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopics, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopics) -> Bool {
    if lhs.entries != rhs.entries {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicId: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaTopicId"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "value"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.value) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.value.isEmpty {
      try visitor.visitSingularStringField(value: self.value, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicId, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicId) -> Bool {
    if lhs.value != rhs.value {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterPassword: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaClusterPassword"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "value"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.value) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.value.isEmpty {
      try visitor.visitSingularStringField(value: self.value, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterPassword, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterPassword) -> Bool {
    if lhs.value != rhs.value {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_AddKafkaTopicsCommandInput: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AddKafkaTopicsCommandInput"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "kafka_cluster_id"),
    2: .standard(proto: "kafka_topics"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.kafkaClusterID) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.kafkaTopics) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.kafkaClusterID.isEmpty {
      try visitor.visitSingularStringField(value: self.kafkaClusterID, fieldNumber: 1)
    }
    if !self.kafkaTopics.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.kafkaTopics, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_AddKafkaTopicsCommandInput, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_AddKafkaTopicsCommandInput) -> Bool {
    if lhs.kafkaClusterID != rhs.kafkaClusterID {return false}
    if lhs.kafkaTopics != rhs.kafkaTopics {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterList: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaClusterList"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "total_pages"),
    2: .same(proto: "entries"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt32Field(value: &self.totalPages) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.entries) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.totalPages != 0 {
      try visitor.visitSingularInt32Field(value: self.totalPages, fieldNumber: 1)
    }
    if !self.entries.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.entries, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterList, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaClusterList) -> Bool {
    if lhs.totalPages != rhs.totalPages {return false}
    if lhs.entries != rhs.entries {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_AddOrUpdateKafkaTopicCommandInput: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AddOrUpdateKafkaTopicCommandInput"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "kafka_cluster_id"),
    2: .standard(proto: "kafka_topic"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.kafkaClusterID) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._kafkaTopic) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.kafkaClusterID.isEmpty {
      try visitor.visitSingularStringField(value: self.kafkaClusterID, fieldNumber: 1)
    }
    try { if let v = self._kafkaTopic {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_AddOrUpdateKafkaTopicCommandInput, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_AddOrUpdateKafkaTopicCommandInput) -> Bool {
    if lhs.kafkaClusterID != rhs.kafkaClusterID {return false}
    if lhs._kafkaTopic != rhs._kafkaTopic {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_DeleteOrRestoreKafkaTopicCommandInput: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".DeleteOrRestoreKafkaTopicCommandInput"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "kafka_cluster_id"),
    2: .standard(proto: "kafka_topic_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.kafkaClusterID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.kafkaTopicID) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.kafkaClusterID.isEmpty {
      try visitor.visitSingularStringField(value: self.kafkaClusterID, fieldNumber: 1)
    }
    if !self.kafkaTopicID.isEmpty {
      try visitor.visitSingularStringField(value: self.kafkaTopicID, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_DeleteOrRestoreKafkaTopicCommandInput, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_DeleteOrRestoreKafkaTopicCommandInput) -> Bool {
    if lhs.kafkaClusterID != rhs.kafkaClusterID {return false}
    if lhs.kafkaTopicID != rhs.kafkaTopicID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicQueryInput: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KafkaTopicQueryInput"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "kafka_cluster_id"),
    2: .standard(proto: "kafka_topic"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.kafkaClusterID) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._kafkaTopic) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if !self.kafkaClusterID.isEmpty {
      try visitor.visitSingularStringField(value: self.kafkaClusterID, fieldNumber: 1)
    }
    try { if let v = self._kafkaTopic {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicQueryInput, rhs: Cloud_Planton_Apis_V1_Code2cloud_Deploy_Kafka_Rpc_KafkaTopicQueryInput) -> Bool {
    if lhs.kafkaClusterID != rhs.kafkaClusterID {return false}
    if lhs._kafkaTopic != rhs._kafkaTopic {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
